# Hadoop 集群安装

## 在 master 节点安装

```bash
# 创建hadoop安装目录
mkdir /usr/local/hadoop
# 解压hadoop安装程序到 hadoop目录
tar -zxf /root/hadoop-2.9.2.tar.gz -C /usr/local/hadoop/

# 配置环境变量
vim /etc/profile
#
# hadoop env
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.9.2
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

source /etc/profile

# 验证hadoop命令
hadoop --help

# 创建临时文件目录
mkdir -p /var/hadoop/tmp
```

## 修改配置文件

```bash
# 配置文件所在目录/usr/local/hadoop/hadoop-2.9.2/etc/hadoop
# 1. 修改 hadoop-env.sh
export JAVA_HOME=/usr/local/java/jdk1.8.0_192

# 2. 修改 yarn-env.sh
export JAVA_HOME=/usr/local/java/jdk1.8.0_192

# 3. 修改 core-site.xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://hadoop-master:8020</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/var/hadoop/tmp</value>
  </property>
</configuration>

# 4. 修改 hdfs-site.xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>hadoop-slave1:50090</value>
  </property>
  <property>
    <name>dfs.permissions</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>
</configuration>

# 5. 修改 mapred-site.xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapred.job.tracker</name>
    <value>hadoop-master:49001</value>
  </property> 
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop-master:10020</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop-master:19888</value>
  </property>
</configuration>

# 6. 修改 yarn-site.xml
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>hadoop-master</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>hadoop-master:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>hadoop-master:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>hadoop-master:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>hadoop-master:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>hadoop-master:8033</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>106800</value>
  </property>
</configuration>


# 7. 修改 slaves、删除文件内的localhost
hadoop-master
hadoop-slave1
hadoop-slave2
```

## 子节点安装

```bash
# 创建hadoop安装目录
mkdir /usr/local/hadoop

# 创建临时文件目录
mkdir -p /var/hadoop/tmp

# 在master节点复制hadoop 到各个子节点
scp -r /usr/local/hadoop/hadoop-2.9.2/ root@hadoop-slave1:/usr/local/hadoop
scp -r /usr/local/hadoop/hadoop-2.9.2/ root@hadoop-slave2:/usr/local/hadoop

# 配置环境变量
vim /etc/profile
#
# hadoop env
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.9.2
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

source /etc/profile

# 验证hadoop命令
hadoop --help
```

## 启动 hadoop

```bash
# 在master节点操作
# 格式化hdfs
hdfs namenode -format //该操作只在集群第一次启动的时执行

# 启动hadoop集群
start-all.sh

# 启动hdfs web接口
httpfs.sh start

# 启动历史任务服务
mr-jobhistory-daemon.sh start historyserver
```

## 验证启动

### 启动的进程

![hadoop-master的进程](assets/image-20211011155014324.png)

![hadoop-slave1的进程](assets/image-20211011155056813.png)

![hadoop-slave2的进程](assets/image-20211011155143006.png)

### 管理网站

1. hdfs 管理 http://hadoop-master:50070
2. mapreduce 管理 http://hadoop-master:8088
3. jobhistory 查看 http://hadoop-master:19888/jobhistory

